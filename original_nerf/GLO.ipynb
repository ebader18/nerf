{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLOModel_CNN(nn.Module):\n",
    "    def __init__(self, latent_dim, output_shape=(3, 400, 400)):\n",
    "        super(GLOModel_CNN, self).__init__()\n",
    "        self.init_size = output_shape[1] // 8  # This will scale down the output shape\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, output_shape[0], 3, stride=1, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        img = nn.functional.interpolate(img, size=(400, 400), mode='bilinear', align_corners=False)\n",
    "        return img\n",
    "    \n",
    "class GLOModel_MLP(nn.Module):\n",
    "    def __init__(self, latent_dim, output_shape=(3, 400, 400)):\n",
    "        super(GLOModel_MLP, self).__init__()\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(256, torch.prod(torch.tensor(output_shape))),\n",
    "        )\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_layers(x)\n",
    "        x = x.view(-1, *self.output_shape)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(directory)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.directory, self.images[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        #plt.imshow(image)\n",
    "        #plt.show()\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return idx, image\n",
    "\n",
    "def compute_mean_std(loader):\n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    total_images_count = 0\n",
    "    for images in loader:\n",
    "        images = images.view(images.size(0), images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        total_images_count += images.size(0)\n",
    "    mean /= total_images_count\n",
    "    std /= total_images_count\n",
    "    return mean, std\n",
    "\n",
    "batch_size = 10\n",
    "temp_loader = DataLoader(\n",
    "    CustomImageDataset(directory='C:/_sw/eb_python/deep_learning/_dataset/NeRF/images/helmet/_temp/train/imgs', \n",
    "                       transform=transforms.Compose([transforms.Resize((400, 400)), transforms.ToTensor()])),\n",
    "    batch_size=batch_size,  # Adjust batch size according to your system's memory\n",
    "    shuffle=False\n",
    ")\n",
    "'''mean, std = compute_mean_std(temp_loader)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((400, 400)),  # Resize to 400x400 if not already\n",
    "    transforms.ToTensor(),  # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(mean=mean.tolist(), std=std.tolist())  # Custom normalization\n",
    "])\n",
    "\n",
    "dataset = CustomImageDataset(\n",
    "    directory='C:/_sw/eb_python/deep_learning/_dataset/NeRF/images/helmet/_temp/train/imgs', \n",
    "    transform=transformations\n",
    ")\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "mean, std = compute_mean_std(data_loader)\n",
    "print(f\"Dataset Mean: {mean}\")\n",
    "print(f\"Dataset Std: {std}\")'''\n",
    "data_loader = temp_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 100\n",
    "latent_dim = 200  # Dimension of the latent space\n",
    "num_images = len(data_loader.dataset)\n",
    "\n",
    "glo_model = GLOModel_CNN(latent_dim).to(device)\n",
    "\n",
    "latent_vectors = nn.Parameter(torch.randn(num_images, latent_dim, device=device, requires_grad=True))\n",
    "optimizer_model = optim.Adam(glo_model.parameters(), lr=1e-4)\n",
    "optimizer_latent = optim.Adam([latent_vectors], lr=1e-4)\n",
    "loss_fn_latent = nn.MSELoss()\n",
    "loss_fn_model = nn.MSELoss()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    for idx_batch, (indices, gt_batch) in enumerate(data_loader):\n",
    "        indices = indices.to(device)\n",
    "        gt_batch = gt_batch.to(device)\n",
    "        for idx, idx_global in enumerate(indices):\n",
    "            optimizer_latent.zero_grad()\n",
    "            latent_vector = latent_vectors[idx_global].unsqueeze(0)\n",
    "            recon_img = glo_model(latent_vector).squeeze(0)\n",
    "            loss_latent = loss_fn_latent(recon_img, gt_batch[idx])\n",
    "            loss_latent.backward()\n",
    "            optimizer_latent.step()\n",
    "        \n",
    "        optimizer_model.zero_grad()\n",
    "        batch_latent_vectors = latent_vectors[indices]\n",
    "        recon_batch = glo_model(batch_latent_vectors)\n",
    "        loss_model = loss_fn_model(recon_batch, gt_batch)\n",
    "        loss_model.backward()\n",
    "        optimizer_model.step()\n",
    "\n",
    "        '''\n",
    "        #real_images = real_images.view(real_images.size(0), -1)\n",
    "        real_images = real_images.view(real_images.size(0), 3, 400, 400)\n",
    "        batch_latent_vectors = latent_vectors[i*data_loader.batch_size:(i+1)*data_loader.batch_size]        \n",
    "        optimizer_model.zero_grad()\n",
    "        optimizer_model.zero_grad()\n",
    "        #generated_images = glo_model(latent_vectors[i])\n",
    "        generated_images = glo_model(batch_latent_vectors)\n",
    "        loss = criterion(generated_images, real_images)\n",
    "        loss.backward()\n",
    "        optimizer_model.step()\n",
    "        optimizer_model.step()'''\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, Loss: {loss_model.item()}')\n",
    "    #plt.imshow(recon_batch[5].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "    #plt.show()\n",
    "    #plt.imshow(gt_batch[0].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "    #plt.show()\n",
    "\n",
    "# Save your model and latent vectors for future use\n",
    "#torch.save(glo_model.state_dict(), 'glo_model.pth')\n",
    "#torch.save(latent_vectors, 'latent_vectors.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(25, 8, figsize=(25, 75))\n",
    "axs = axs.flatten()\n",
    "count = 0\n",
    "\n",
    "for idx_batch, (indices, gt_batch) in enumerate(data_loader):\n",
    "    if count >= 200:  # Stop if we have filled up our grid\n",
    "        break\n",
    "    \n",
    "    indices = indices.to(device)\n",
    "    gt_batch = gt_batch.to(device)\n",
    "    \n",
    "    for idx, idx_global in enumerate(indices):\n",
    "        if count >= 200:  # Check again inside the loop\n",
    "            break\n",
    "        \n",
    "        # Generate the reconstructed image\n",
    "        latent_vector = latent_vectors[idx_global].unsqueeze(0)\n",
    "        recon_img = glo_model(latent_vector).squeeze(0)\n",
    "        \n",
    "        # Plot the ground truth image\n",
    "        axs[count].imshow(gt_batch[idx].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "        axs[count].axis('off')  # Remove axis for clarity\n",
    "        count += 1  # Increment counter\n",
    "        \n",
    "        if count >= 200:  # Check again after plotting the real image\n",
    "            break\n",
    "        \n",
    "        # Plot the reconstructed image\n",
    "        axs[count].imshow(recon_img.detach().cpu().numpy().transpose(1, 2, 0))\n",
    "        axs[count].axis('off')  # Remove axis for clarity\n",
    "        count += 1  # Increment counter\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "'''for idx_batch, (indices, gt_batch) in enumerate(data_loader):\n",
    "    indices = indices.to(device)\n",
    "    gt_batch = gt_batch.to(device)\n",
    "    for idx, idx_global in enumerate(indices):\n",
    "        latent_vector = latent_vectors[idx_global].unsqueeze(0)\n",
    "        recon_img = glo_model(latent_vector).squeeze(0)\n",
    "        plt.imshow(gt_batch[idx].detach().cpu().numpy().transpose(1, 2, 0))\n",
    "        plt.show()\n",
    "        plt.imshow(recon_img.squeeze(0).detach().cpu().numpy().transpose(1, 2, 0))\n",
    "        plt.show()'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Standard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
